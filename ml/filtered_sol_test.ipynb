{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf939483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 00:19:24.553928: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-16 00:19:24.745320: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-16 00:19:25.489540: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/gmp/6.2.1/b1/lib:/software/glpk/4.65/lib:/software/zmq/4.2.3/b1/lib:/software/git/2.30.1/lib64:/software/gcc/7.3.0/lib64:/software/gcc/7.3.0/lib:/software/openmpi/4.0.4/b1/lib:/software/cuda/11.4/usr/local/cuda-11.4/lib64:/software/cuda/11.4/usr/local/cuda-11.4/targets/x86_64-linux/lib:/software/cudnn/11.4-8.2.4.15/lib64:/software/slurm/current/lib64:/software/slurm/current/lib\n",
      "2022-12-16 00:19:25.489853: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/gmp/6.2.1/b1/lib:/software/glpk/4.65/lib:/software/zmq/4.2.3/b1/lib:/software/git/2.30.1/lib64:/software/gcc/7.3.0/lib64:/software/gcc/7.3.0/lib:/software/openmpi/4.0.4/b1/lib:/software/cuda/11.4/usr/local/cuda-11.4/lib64:/software/cuda/11.4/usr/local/cuda-11.4/targets/x86_64-linux/lib:/software/cudnn/11.4-8.2.4.15/lib64:/software/slurm/current/lib64:/software/slurm/current/lib\n",
      "2022-12-16 00:19:25.489866: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import urllib\n",
    "import tensorflowjs as tfjs\n",
    "import json\n",
    "import keras\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b63ee0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  4\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cc4cfe",
   "metadata": {},
   "source": [
    "## Downloading solubility model from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74b6ac83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 00:19:33.512034: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-16 00:19:35.640366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10786 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:08:00.0, compute capability: 3.7\n",
      "2022-12-16 00:19:35.641647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 10786 MB memory:  -> device: 1, name: Tesla K80, pci bus id: 0000:09:00.0, compute capability: 3.7\n",
      "2022-12-16 00:19:35.643294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 10786 MB memory:  -> device: 2, name: Tesla K80, pci bus id: 0000:88:00.0, compute capability: 3.7\n",
      "2022-12-16 00:19:35.644429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 10786 MB memory:  -> device: 3, name: Tesla K80, pci bus id: 0000:89:00.0, compute capability: 3.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk.\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('model_weights_seeded_SOL.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model.load_weights(\"model_weights_seeded_SOL.h5\")\n",
    "print(\"Loaded model from disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddea335a",
   "metadata": {},
   "source": [
    "# Getting test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fec1c7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\n",
    "    \"https://github.com/ur-whitelab/peptide-dashboard/raw/master/ml/data/insoluble.npz\",\n",
    "    \"insoluble.npz\",\n",
    ")\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://github.com/ur-whitelab/peptide-dashboard/raw/master/ml/data/soluble.npz\",\n",
    "    \"soluble.npz\",\n",
    ")\n",
    "with np.load(\"soluble.npz\") as r:\n",
    "    pos_data = r['arr_0']\n",
    "with np.load(\"insoluble.npz\") as r:\n",
    "    neg_data = r['arr_0']\n",
    "\n",
    "def counts_aa(vec):\n",
    "    counts =  tf.histogram_fixed_width(vec, [0, 20], nbins=21)[1:]\n",
    "    return counts /tf.reduce_sum(counts)\n",
    "labels = np.concatenate(\n",
    "    (\n",
    "        np.ones((pos_data.shape[0], 1), dtype=pos_data.dtype),\n",
    "        np.zeros((neg_data.shape[0], 1), dtype=pos_data.dtype),\n",
    "    ),\n",
    "    axis=0,\n",
    ")\n",
    "features = np.concatenate((pos_data, neg_data), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d02de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    vocab_size: int\n",
    "    example_number: int\n",
    "    batch_size: int\n",
    "    buffer_size: int\n",
    "    rnn_units: int\n",
    "    hidden_dim: int\n",
    "    embedding_dim: int\n",
    "    reg_strength: float\n",
    "    lr: float\n",
    "    drop_rate: float\n",
    "        \n",
    "config = Config(vocab_size=21, # include gap\n",
    "                example_number=len(labels), \n",
    "                batch_size=16, \n",
    "                buffer_size=10000,\n",
    "                rnn_units=64,\n",
    "                hidden_dim=64,\n",
    "                embedding_dim=32,\n",
    "                reg_strength=0.01,\n",
    "                lr=1e-4,\n",
    "                drop_rate=0.2\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d89a5026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now need to shuffle before creating TF dataset\n",
    "# so that our train/test/val splits are random\n",
    "np.random.seed(0) # Note: seed 0 is used for training. DO NOT CHANGE!\n",
    "                                 \n",
    "i = np.arange(len(labels))\n",
    "np.random.shuffle(i)\n",
    "shuffled_labels = labels[i]\n",
    "shuffled_features = features[i]\n",
    "data = tf.data.Dataset.from_tensor_slices((shuffled_features, shuffled_labels)).map(lambda x,y: ((x, counts_aa(x)), y))\n",
    "# now split into val, test, train and batch\n",
    "N = len(data)  \n",
    "L = None#features[0].shape[-1]\n",
    "split = int(0.1 * N)\n",
    "test_data = data.take(split).batch(config.batch_size)\n",
    "nontest = data.skip(split)\n",
    "val_data, train_data = nontest.take(split).batch(config.batch_size), \\\n",
    "    nontest.skip(split).shuffle(config.buffer_size).batch(config.batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3844dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = shuffled_features[:split]\n",
    "y_test = shuffled_labels[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef147987",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_tpu = False\n",
    "decay_epochs = 50\n",
    "decay_steps = N  // config.batch_size * decay_epochs\n",
    "lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n",
    "  config.lr, decay_steps, alpha=1e-3)\n",
    "opt = tf.optimizers.Adam(lr_decayed_fn)\n",
    "model.compile(\n",
    "  opt,\n",
    "  loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "  steps_per_execution = 60 if use_tpu else None,\n",
    "  metrics=[tf.keras.metrics.AUC(from_logits=False), tf.keras.metrics.BinaryAccuracy(threshold=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ccb7f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 3s 27ms/step\n",
      "Best Threshold=0.516320, G-Mean=0.697\n",
      "Accuracy: 0.710\n"
     ]
    }
   ],
   "source": [
    "y_hat_test = model.predict(test_data)\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_hat_test, drop_intermediate=False)\n",
    "# calculate the g-mean for each threshold\n",
    "gmeans = np.sqrt(tpr * (1-fpr))\n",
    "# locate the index of the largest g-mean\n",
    "ix = np.argmax(gmeans)\n",
    "best_accuracy_threshold = thresholds[ix]\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n",
    "adjusted_y_hat_test = [1 if m>best_accuracy_threshold else 0 for m in y_hat_test]\n",
    "acc = accuracy_score(y_test, adjusted_y_hat_test, normalize=True)\n",
    "print(f'Accuracy: {acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efa06b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model again based on adjusted decision boundary threshold\n",
    "model.compile(\n",
    "  opt,\n",
    "  loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "  steps_per_execution = 60 if use_tpu else None,\n",
    "  metrics=[tf.keras.metrics.AUC(from_logits=False), tf.keras.metrics.BinaryAccuracy(threshold=best_accuracy_threshold)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4465c389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lengths(X,y, min_length=10, max_length=80):\n",
    "    test_lengths = np.count_nonzero(X, axis=1)\n",
    "    filtered_idx = np.where((test_lengths>min_length) & (test_lengths<max_length))\n",
    "    return X[filtered_idx], y[filtered_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122ecea5",
   "metadata": {},
   "source": [
    "## No length filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8824b2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 8s 32ms/step - loss: 0.5770 - auc_1: 0.7561 - binary_accuracy: 0.7100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5769950747489929, 0.7561261057853699, 0.7100270986557007]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_f, y_test_f = filter_lengths(X_test, y_test, min_length=1, max_length=200)\n",
    "\n",
    "filtered_test_data = tf.data.Dataset.from_tensor_slices((X_test_f, y_test_f)).map(lambda x,y: ((x, counts_aa(x)), y))\n",
    "filtered_test_data = filtered_test_data.batch(config.batch_size)\n",
    "model.evaluate(filtered_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431419a",
   "metadata": {},
   "source": [
    "## Length filter 1 - 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2793fba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2229 - auc_1: 0.9524 - binary_accuracy: 0.9130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22285817563533783, 0.9523809552192688, 0.9130434989929199]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_f, y_test_f = filter_lengths(X_test, y_test, min_length=1, max_length=50)\n",
    "\n",
    "filtered_test_data = tf.data.Dataset.from_tensor_slices((X_test_f, y_test_f)).map(lambda x,y: ((x, counts_aa(x)), y))\n",
    "filtered_test_data = filtered_test_data.batch(config.batch_size)\n",
    "model.evaluate(filtered_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391c8029",
   "metadata": {},
   "source": [
    "## Length filter 50 - 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "775aaec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 20ms/step - loss: 0.5422 - auc_1: 0.7949 - binary_accuracy: 0.7243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5421819090843201, 0.7949349880218506, 0.7242646813392639]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_f, y_test_f = filter_lengths(X_test, y_test, min_length=50, max_length=100)\n",
    "\n",
    "filtered_test_data = tf.data.Dataset.from_tensor_slices((X_test_f, y_test_f)).map(lambda x,y: ((x, counts_aa(x)), y))\n",
    "filtered_test_data = filtered_test_data.batch(config.batch_size)\n",
    "model.evaluate(filtered_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fc1c8",
   "metadata": {},
   "source": [
    "## Length filter 100 - 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "555b49bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 1s 26ms/step - loss: 0.5798 - auc_1: 0.7549 - binary_accuracy: 0.7035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5798361897468567, 0.7549368143081665, 0.703496515750885]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_f, y_test_f = filter_lengths(X_test, y_test, min_length=100, max_length=150)\n",
    "\n",
    "filtered_test_data = tf.data.Dataset.from_tensor_slices((X_test_f, y_test_f)).map(lambda x,y: ((x, counts_aa(x)), y))\n",
    "filtered_test_data = filtered_test_data.batch(config.batch_size)\n",
    "model.evaluate(filtered_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b4a4e1",
   "metadata": {},
   "source": [
    "## Length filter 150 - 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94c7bf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 2s 33ms/step - loss: 0.6006 - auc_1: 0.7352 - binary_accuracy: 0.7022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6005921959877014, 0.7351933717727661, 0.7022332549095154]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_f, y_test_f = filter_lengths(X_test, y_test, min_length=150, max_length=200)\n",
    "\n",
    "filtered_test_data = tf.data.Dataset.from_tensor_slices((X_test_f, y_test_f)).map(lambda x,y: ((x, counts_aa(x)), y))\n",
    "filtered_test_data = filtered_test_data.batch(config.batch_size)\n",
    "model.evaluate(filtered_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7970bf68",
   "metadata": {},
   "source": [
    "## Length filter 1 - 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b416d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 19ms/step - loss: 0.5173 - auc_1: 0.8138 - binary_accuracy: 0.7390\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5172854661941528, 0.8138336539268494, 0.7389830350875854]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_f, y_test_f = filter_lengths(X_test, y_test, min_length=1, max_length=100)\n",
    "\n",
    "filtered_test_data = tf.data.Dataset.from_tensor_slices((X_test_f, y_test_f)).map(lambda x,y: ((x, counts_aa(x)), y))\n",
    "filtered_test_data = filtered_test_data.batch(config.batch_size)\n",
    "model.evaluate(filtered_test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "serverless"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
